{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前準備\n",
    "### 元のテキストデータは以下からダウンロードしてください。\n",
    "https://www.rondhuit.com/download.html#ldcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnz = Tokenizer()\n",
    "pth = pathlib.Path('c:/temp/text')\n",
    "\n",
    "l = []\n",
    "for p in pth.glob('**/*.txt') :\n",
    "    # 記事データ以外はスキップ\n",
    "    if p.name in ['CHANGES.txt','README.txt','LICENSE.txt']:\n",
    "        continue\n",
    "\n",
    "    # 記事データを開き、janomeで形態素解析⇒1行1単語の形式でリストに保持\n",
    "    with open(p,'r',encoding='utf-8-sig') as f :\n",
    "        l.extend([[p.parent.name, p.name, t.surface, t.part_of_speech] for s in f for t in tnz.tokenize(s)])\n",
    "\n",
    "# リストをデータフレームに変換\n",
    "df = pd.DataFrame(np.array(l))\n",
    "\n",
    "# 列名を付与\n",
    "df.columns = ['記事分類','ファイル名','単語','品詞']\n",
    "\n",
    "# データフレームをcsv出力\n",
    "df.to_csv('c:/temp/livedoor_corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記事分類毎の頻出単語Top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前準備で1ファイルにまとめたテキストデータを読み込む\n",
    "df = pd.read_csv('c:/temp/livedoor_corpus.csv')\n",
    "\n",
    "# 品詞は名詞一般のみに絞り込み\n",
    "df = df[df['品詞'].str.startswith('名詞,一般')].reset_index(drop=True)\n",
    "\n",
    "# 記事分類毎の単語の出現頻度を集計\n",
    "gdf = pd.crosstab([df['記事分類'],df['単語'],df['品詞']],\n",
    "                  '件数',\n",
    "                  aggfunc='count',\n",
    "                  values=df['単語']\n",
    "                 ).reset_index()\n",
    "\n",
    "# 記事分類毎の単語の出現頻度の降順で順位付け\n",
    "gdf['順位'] = gdf.groupby(['記事分類'])['件数'].rank('dense', ascending=False)\n",
    "gdf.sort_values(['記事分類','順位'], inplace=True)\n",
    "\n",
    "# 記事分類毎の頻出単語Top5だけに絞り込み\n",
    "gdf = gdf[gdf['順位'] <= 5]\n",
    "\n",
    "# 記事分類毎の頻出単語Top5の確認\n",
    "for k in gdf['記事分類'].unique():\n",
    "    display(gdf[gdf['記事分類']==k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 気になる単語の周辺文の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 気になった単語を設定\n",
    "word = 'T'\n",
    "\n",
    "df = pd.read_csv('c:/temp/livedoor_corpus.csv')\n",
    "\n",
    "# 気になった単語の出現位置を取得\n",
    "idxes = df[(df['単語'] == word)\n",
    "          &(df['品詞'].str.startswith('名詞,一般'))].index.values.tolist()\n",
    "\n",
    "# ウィンドウサイズ（気になった単語の前後何単語まで確認するかの設定）\n",
    "ws = 20\n",
    "\n",
    "# 気になる単語の周辺文を取得\n",
    "l = []\n",
    "for i, r in df.loc[idxes, :].iterrows():\n",
    "    s = i - ws\n",
    "    e = i + ws\n",
    "    tmp = df.loc[s:e, :]\n",
    "    tmp = tmp[tmp['ファイル名']==r['ファイル名']]\n",
    "    lm = list(map(str, tmp['単語'].values.tolist()))\n",
    "    ss = ''.join(lm)\n",
    "    l.append([r['記事分類'],r['ファイル名'],r['単語'],ss])\n",
    "rdf = pd.DataFrame(np.array(l))\n",
    "rdf.columns = ['記事分類','ファイル名','単語','単語周辺文']\n",
    "\n",
    "rdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストデータ加工（空白、改行、URL除去）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "tnz = Tokenizer()\n",
    "\n",
    "pth = pathlib.Path('c:/temp/text')\n",
    "\n",
    "l = []\n",
    "for p in pth.glob('**/*.txt') :\n",
    "    # 記事データ以外はスキップ\n",
    "    if p.name in ['CHANGES.txt','README.txt','LICENSE.txt']:\n",
    "        continue\n",
    "    \n",
    "    # 記事データを開き、janomeで形態素解析⇒1行1単語の形式でリストに保持\n",
    "    with open(p,'r',encoding='utf-8-sig') as f :\n",
    "        s = f.read()\n",
    "        s = s.replace('　', '')\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('\\n', '')\n",
    "        s = re.sub(r'http://.*\\+[0-9]{4}', '', s)\n",
    "        # 空白、改行、URLを除去\n",
    "        l.extend([[p.parent.name, p.name, t.surface, t.part_of_speech] for t in tnz.tokenize(s)])\n",
    "\n",
    "# リストをデータフレームに変換\n",
    "df = pd.DataFrame(np.array(l))\n",
    "\n",
    "# 列名を付与\n",
    "df.columns = ['記事分類','ファイル名','単語','品詞']\n",
    "\n",
    "# データフレームをcsv出力\n",
    "df.to_csv('c:/temp/livedoor_corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記事分類毎の頻出単語Top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前準備で1ファイルにまとめたテキストデータを読み込む\n",
    "df = pd.read_csv('c:/temp/livedoor_corpus.csv')\n",
    "\n",
    "# 品詞は名詞一般のみに絞り込み\n",
    "df = df[df['品詞'].str.startswith('名詞,一般')].reset_index(drop=True)\n",
    "\n",
    "# 記事分類毎の単語の出現頻度を集計\n",
    "gdf = pd.crosstab([df['記事分類'],df['単語'],df['品詞']],\n",
    "                  '件数',\n",
    "                  aggfunc='count',\n",
    "                  values=df['単語']\n",
    "                 ).reset_index()\n",
    "\n",
    "# 記事分類毎の単語の出現頻度の降順で順位付け\n",
    "gdf['順位'] = gdf.groupby(['記事分類'])['件数'].rank('dense', ascending=False)\n",
    "gdf.sort_values(['記事分類','順位'], inplace=True)\n",
    "\n",
    "# 記事分類毎の頻出単語Top5だけに絞り込み\n",
    "gdf = gdf[gdf['順位'] <= 5]\n",
    "\n",
    "# 記事分類毎の頻出単語Top5の確認\n",
    "for k in gdf['記事分類'].unique():\n",
    "    display(gdf[gdf['記事分類']==k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストデータの主成分分析（2次元）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上段のセルにて取得した記事分類毎の頻出単語Top5をリストとして保持\n",
    "words = gdf['単語'].unique().tolist()\n",
    "\n",
    "df = pd.read_csv('c:/temp/livedoor_corpus.csv')\n",
    "df = df[df['品詞'].str.startswith('名詞,一般')].reset_index(drop=True)\n",
    "df = df[df['単語'].isin(words)]\n",
    "\n",
    "# ファイルと記事分類毎の頻出単語Top5のクロス集計表を取得\n",
    "xdf = pd.crosstab([df['記事分類'],df['ファイル名']],df['単語']).reset_index()\n",
    "# 後に因子負荷量のラベルとして出力するため、リストとして保持\n",
    "cls = xdf.columns.values.tolist()[2:]\n",
    "\n",
    "# 後のグラフ表示のため、記事分類毎に分類番号を付与\n",
    "ul = xdf['記事分類'].unique()\n",
    "def _fnc(x):\n",
    "    return ul.tolist().index(x)\n",
    "xdf['分類番号'] = xdf['記事分類'].apply(lambda x : _fnc(x))\n",
    "\n",
    "# 主成分を求めるための前準備\n",
    "data = xdf.values\n",
    "labels = data[:,0]\n",
    "d = data[:, 2:-1].astype(np.int64)\n",
    "k = data[:, -1].astype(np.int64)\n",
    "\n",
    "# データの標準化 ※ 標準偏差は不偏標準偏差で計算\n",
    "X = (d - d.mean(axis=0)) / d.std(ddof=1,axis=0)\n",
    "\n",
    "# 相関行列を求めます\n",
    "XX = np.round(np.dot(X.T,X) / (len(X) - 1), 2)\n",
    "\n",
    "# 相関行列の固有値、固有値ベクトルを求めます\n",
    "w, V = np.linalg.eig(XX)\n",
    "\n",
    "# 第1主成分を求める\n",
    "z1 = np.dot(X,V[:,0])\n",
    "\n",
    "# 第2主成分を求める\n",
    "z2 = np.dot(X,V[:,1])\n",
    "\n",
    "# グラフ用オブジェクトの生成\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# グリッド線を入れる\n",
    "ax.grid()\n",
    "\n",
    "# 描画するデータの境界\n",
    "lim = [-10.0, 10.0]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "\n",
    "# 左と下の軸線を真ん中に持っていく\n",
    "ax.spines['bottom'].set_position(('axes', 0.5))\n",
    "ax.spines['left'].set_position(('axes', 0.5))\n",
    "# 右と上の軸線を消す\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 軸の目盛の間隔を調整\n",
    "ticks = np.arange(-10.0, 10.0, 1.0)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "# 軸ラベルの追加、位置の調整\n",
    "ax.set_xlabel('Z1', fontsize=16)\n",
    "ax.set_ylabel('Z2', fontsize=16, rotation=0)\n",
    "ax.xaxis.set_label_coords(1.02, 0.49)\n",
    "ax.yaxis.set_label_coords(0.5, 1.02)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['red','blue','gold','olive','green','dodgerblue','brown','black','grey']\n",
    "cmap = ListedColormap(colors)\n",
    "a = np.array(list(zip(z1,z2,k,labels)))\n",
    "df = pd.DataFrame({'z1':pd.Series(z1, dtype='float'),\n",
    "                   'z2':pd.Series(z2, dtype='float'),\n",
    "                   'k':pd.Series(k, dtype='int'),\n",
    "                   'labels':pd.Series(labels, dtype='str'),\n",
    "                  })\n",
    "\n",
    "# 記事分類毎に色を変えてプロット\n",
    "for l in df['labels'].unique():\n",
    "    d = df[df['labels']==l]\n",
    "    ax.scatter(d['z1'],d['z2'],c=cmap(d['k']),label=l)\n",
    "    ax.legend()\n",
    "\n",
    "# 描画\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因子負荷量（2次元）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大の固有値に対応する固有ベクトルを横軸、最大から2番目の固有値に対応する固有ベクトルを縦軸とした座標。\n",
    "V_ = np.array([(V[:,0]),V[:,1]]).T\n",
    "V_ = np.round(V_,2)\n",
    "\n",
    "# グラフ描画用のデータ\n",
    "z1 = V_[:,0]\n",
    "z2 = V_[:,1]\n",
    "\n",
    "# グラフ用オブジェクトの生成\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# グリッド線を入れる\n",
    "ax.grid()\n",
    "\n",
    "# 描画するデータの境界\n",
    "lim = [-0.4, 0.4]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "\n",
    "# 左と下の軸線を真ん中に持っていく\n",
    "ax.spines['bottom'].set_position(('axes', 0.5))\n",
    "ax.spines['left'].set_position(('axes', 0.5))\n",
    "# 右と上の軸線を消す\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 軸の目盛の間隔を調整\n",
    "ticks = np.arange(-0.4, 0.4, 0.05)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "# 軸ラベルの追加、位置の調整\n",
    "ax.set_xlabel('Z1', fontsize=16)\n",
    "ax.set_ylabel('Z2', fontsize=16, rotation=0)\n",
    "ax.xaxis.set_label_coords(1.02, 0.49)\n",
    "ax.yaxis.set_label_coords(0.5, 1.02)\n",
    "\n",
    "# データのプロット\n",
    "for (i,j,k) in zip(z1,z2,cls):\n",
    "    ax.plot(i,j,'o')\n",
    "    ax.annotate(k, xy=(i, j),fontsize=10)\n",
    "    \n",
    "# 描画\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストデータの主成分分析（3次元）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上段のセルにて取得した記事分類毎の頻出単語Top5をリストとして保持\n",
    "words = gdf['単語'].unique().tolist()\n",
    "\n",
    "df = pd.read_csv('c:/temp/livedoor_corpus.csv')\n",
    "df = df[df['品詞'].str.startswith('名詞,一般')].reset_index(drop=True)\n",
    "df = df[df['単語'].isin(words)]\n",
    "\n",
    "# ファイルと記事分類毎の頻出単語Top5のクロス集計表を取得\n",
    "xdf = pd.crosstab([df['記事分類'],df['ファイル名']],df['単語']).reset_index()\n",
    "# 後に因子負荷量のラベルとして出力するため、リストとして保持\n",
    "cls = xdf.columns.values.tolist()[2:]\n",
    "\n",
    "# 後のグラフ表示のため、記事分類毎に分類番号を付与\n",
    "ul = xdf['記事分類'].unique()\n",
    "def _fnc(x):\n",
    "    return ul.tolist().index(x)\n",
    "xdf['分類番号'] = xdf['記事分類'].apply(lambda x : _fnc(x))\n",
    "\n",
    "# 主成分を求めるための前準備\n",
    "data = xdf.values\n",
    "labels = data[:,0]\n",
    "d = data[:, 2:-1].astype(np.int64)\n",
    "k = data[:, -1].astype(np.int64)\n",
    "\n",
    "# データの標準化 ※ 標準偏差は不偏標準偏差で計算\n",
    "X = (d - d.mean(axis=0)) / d.std(ddof=1,axis=0)\n",
    "\n",
    "# 相関行列を求めます\n",
    "XX = np.round(np.dot(X.T,X) / (len(X) - 1), 2)\n",
    "\n",
    "# 相関行列の固有値、固有値ベクトルを求めます\n",
    "w, V = np.linalg.eig(XX)\n",
    "\n",
    "# 第1主成分を求める\n",
    "z1 = np.dot(X,V[:,0])\n",
    "\n",
    "# 第2主成分を求める\n",
    "z2 = np.dot(X,V[:,1])\n",
    "\n",
    "# 第3主成分を求める\n",
    "z3 = np.dot(X,V[:,2])\n",
    "\n",
    "# グラフ用オブジェクトの生成\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# グリッド線を入れる\n",
    "ax.grid()\n",
    "\n",
    "# 描画するデータの境界\n",
    "lim = [-10.0, 10.0]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "\n",
    "# 左と下の軸線を真ん中に持っていく\n",
    "ax.spines['bottom'].set_position(('axes', 0.5))\n",
    "ax.spines['left'].set_position(('axes', 0.5))\n",
    "# 右と上の軸線を消す\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 軸の目盛の間隔を調整\n",
    "ticks = np.arange(-10.0, 10.0, 1.0)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "# 軸ラベルの追加、位置の調整\n",
    "ax.set_xlabel('Z1', fontsize=16)\n",
    "ax.set_ylabel('Z2', fontsize=16, rotation=0)\n",
    "ax.xaxis.set_label_coords(1.02, 0.49)\n",
    "ax.yaxis.set_label_coords(0.5, 1.02)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['red','blue','gold','olive','green','dodgerblue','brown','black','grey']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "a = np.array(list(zip(z1,z2,z3,k,labels)))\n",
    "df = pd.DataFrame({'z1':pd.Series(z1, dtype='float'),\n",
    "                   'z2':pd.Series(z2, dtype='float'),\n",
    "                   'z3':pd.Series(z3, dtype='float'),\n",
    "                   'k':pd.Series(k, dtype='int'),\n",
    "                   'labels':pd.Series(labels, dtype='str'),\n",
    "                  })\n",
    "\n",
    "for l in df['labels'].unique():\n",
    "    d = df[df['labels']==l]\n",
    "    ax.scatter(d['z1'],d['z2'],d['z3'],c=cmap(d['k']),label=l)\n",
    "    ax.legend()\n",
    "\n",
    "# 描画\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因子負荷量（3次元）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大の固有値に対応する固有ベクトルを横軸、最大から2番目の固有値に対応する固有ベクトルを縦軸とした座標。\n",
    "V_ = np.array([(V[:,0]),V[:,1],V[:,2]]).T\n",
    "V_ = np.round(V_,2)\n",
    "\n",
    "# グラフ描画用のデータ\n",
    "z1 = V_[:,0]\n",
    "z2 = V_[:,1]\n",
    "z3 = V_[:,2]\n",
    "\n",
    "# グラフ用オブジェクトの生成\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# グリッド線を入れる\n",
    "ax.grid()\n",
    "\n",
    "# 描画するデータの境界\n",
    "lim = [-0.4, 0.4]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "\n",
    "# 左と下の軸線を真ん中に持っていく\n",
    "ax.spines['bottom'].set_position(('axes', 0.5))\n",
    "ax.spines['left'].set_position(('axes', 0.5))\n",
    "# 右と上の軸線を消す\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 軸の目盛の間隔を調整\n",
    "ticks = np.arange(-0.4, 0.4, 0.05)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "# 軸ラベルの追加、位置の調整\n",
    "ax.set_xlabel('Z1', fontsize=16)\n",
    "ax.set_ylabel('Z2', fontsize=16)\n",
    "ax.set_zlabel('Z3', fontsize=16)\n",
    "\n",
    "ax.xaxis.set_label_coords(1.02, 0.49)\n",
    "ax.yaxis.set_label_coords(0.5, 1.02)\n",
    "\n",
    "# データのプロット\n",
    "for zdir, x, y, z in zip(cls, z1, z2, z3):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, zdir)\n",
    "\n",
    "# 描画\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
